<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />



<meta name="progressive" content="true" />
<meta name="allow-skip" content="true" />

<title>Lecture: Bayesian Fundamentals</title>


<!-- highlightjs -->
<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>

<!-- taken from https://github.com/rstudio/rmarkdown/blob/67b7f5fc779e4cfdfd0f021d3d7745b6b6e17149/inst/rmd/h/default.html#L296-L362 -->
<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<script>
$(document).ready(function () {
  window.buildTabsets("section-TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>
<!-- end tabsets -->


<link rel="stylesheet" href="css\learnr-theme.css" type="text/css" />

</head>

<body>



<div class="pageContent band">
<div class="bandContent page">

<div class="topics">

<div id="section-bayesian-fundamentals" class="section level2">
<h2>Bayesian Fundamentals</h2>
<div id="section-the-punchline" class="section level3">
<h3>The punchline</h3>
<blockquote>
<p><sub> In the Bayesian world the unobserved quantities are assigned distributional properties and, therefore, become random variables in the analysis. </sub> <br></p>
<p><sub> These distributions come in two basic flavors. If the distribution of the unknown quantity is not conditioned on fixed data, it is called prior distribution because it describes knowledge prior to seeing data. </sub> <br></p>
<p><sub> Alternatively, if the distribution is conditioned on data that we observe, it is clearly updated from the unconditioned state and, therefore, more informed. This distribution is called posterior distribution. […] </sub> <br></p>
<p><sub> The punchline is this: All likelihood-based models are Bayesian models in which the prior distribution is an appropriately selected uniform prior, and as the size of the data gets large they are identical given any finite appropriate prior. So such empirical researchers are really Bayesian; they just do not know it yet. </sub></p>
</blockquote>
<div style="text-align: right">
<p><sub><sup> <a href="https://academic.oup.com/jpart/article/23/2/457/1003493">Gill, J., &amp; Witko, C. (2013). Bayesian analytical methods: A methodological prescription for public administration. Journal of Public Administration Research and Theory, 23(2), 457–494.</a> </sub></sup></p>
</div>
</div>
<div id="section-likelihood-function" class="section level3">
<h3>Likelihood function</h3>
<ul>
<li>Specification of a pdf or pmf: <span class="math inline">\(p(\mathbf{y}|\theta)\)</span>.</li>
<li>Also called the data generating process (or the generative model) for <span class="math inline">\(y\)</span>.</li>
<li>Logical inversion: “Which unknown <span class="math inline">\(\theta\)</span> most likely produces the known <span class="math inline">\(\mathbf{y}\)</span>?” <span class="math inline">\(\rightarrow\)</span> <span class="math inline">\(L(\theta | \mathbf{y})\)</span>.</li>
<li>The notational distinction between <span class="math inline">\(p(\mathbf{y}|\theta)\)</span> and <span class="math inline">\(L(\theta | \mathbf{y})\)</span> is purely conceptual. <span class="math inline">\(p(\mathbf{y}|\theta) = L(\theta | \mathbf{y})\)</span>.</li>
<li>We will use <span class="math inline">\(p(\mathbf{y}|\theta)\)</span>.</li>
<li>Note that the likelihood function multiplies densities across <em>all</em> observations; e.g., a normal likelihood function is given by:</li>
</ul>
<p><span class="math display">\[p(\mathbf{y}|\mu, \sigma) = \prod_{i=1}^{N} \frac{1}{\sigma \sqrt{2 \pi}} e^{- 0.5 \left( (y_i - \mu_i)^2 / \sigma \right)}\]</span></p>
<ul>
<li>This is what we mean mathematically when we use the shorthand
<ul>
<li><span class="math inline">\(\mathbf{y} \sim \text{N}(\mu, \sigma)\)</span> or</li>
<li><span class="math inline">\(y_i \sim \text{N}(\mu_i, \sigma) \text{ for all } i=1,...N\)</span>.</li>
</ul></li>
</ul>
</div>
<div id="section-prior-distribution" class="section level3">
<h3>Prior distribution</h3>
<ul>
<li>A distributional characterization of our belief about an unknown quantity (i.e., a parameter) prior to seeing the data: <span class="math inline">\(p(\theta)\)</span></li>
<li>This includes statements about <em>family</em>, <em>support</em>, and <em>density</em>.
<ul>
<li><em>Family</em>: A pdf (continuous parameters) or pmf (discrete parameters) that can plausibly generate the parameter values.</li>
<li><em>Support</em>: Some parameters have constrained support: Probability parameters must be inside <span class="math inline">\([0, 1]\)</span>; variance parameters must be <span class="math inline">\(\geq 0\)</span>.</li>
<li><em>Density</em>: A distributional characterization which values of the parameter we think are more or less likely to observe.</li>
</ul></li>
<li>The prior distribution can be
<ul>
<li>flat (i.e., uniformly distributed over the supported range – often improper)</li>
<li>purposefully very vague, and thus, rather uninformative</li>
<li>weakly informative</li>
<li>specific and substantively informed (e.g., by previous research or expert assessment)</li>
</ul></li>
</ul>
</div>
<div id="section-posterior-distribution" class="section level3">
<h3>Posterior distribution</h3>
<ul>
<li>Updating our distributional belief about <span class="math inline">\(\theta\)</span> given the data, <span class="math inline">\(\mathbf{y}\)</span>: <span class="math inline">\(p(\theta | \mathbf{y})\)</span></li>
<li>Follows the proportional version of <a href="https://en.wikipedia.org/wiki/Bayes%27_theorem">Bayes’ Law</a>: <span class="math inline">\(p(\theta | \mathbf{y}) \propto p(\theta) \times p(\mathbf{y}|\theta)\)</span></li>
<li>Yields a weighthed combination of likelihood and prior</li>
<li>The prior pulls the posterior density toward the center of gravity of the prior distribution</li>
<li>As the data grows large, the likelihood becomes more influential:
<ul>
<li>one factor for <span class="math inline">\(p(\theta)\)</span>, <span class="math inline">\(N\)</span> factors for <span class="math inline">\(p(y_i|\theta_i)\)</span></li>
<li>we will see this analytically and using simulations later on</li>
</ul></li>
</ul>
</div>
</div>
<div id="section-coin-flip-experiment" class="section level2">
<h2>Coin flip experiment</h2>
<div id="section-the-experiment" class="section level3">
<h3>The experiment</h3>
<p>Suppose we flip a coin up to <span class="math inline">\(N\)</span> times:</p>
<ul>
<li>The fairness of a coin can be expressed through a <em>probability parameter</em>, <span class="math inline">\(\pi\)</span>, that governs the probability that a coin flip produces heads (H) has apposed to tails</li>
<li>We start out with the believe that the coin is fair – that is, we consider it more probable that the coin is fair (<span class="math inline">\(\pi \approx 0.5\)</span>) and less probable that it over-produces heads or tails</li>
<li>Unbeknownst to us, the coin is far from fair – it is 4 times as likely to produce heads as it is to produce tails (that is, <span class="math inline">\(\pi=0.8\)</span>)</li>
<li>We slowly learn about this in the process of flipping the coin and keeping score of the number of flips <span class="math inline">\(n\)</span> and the number of heads <span class="math inline">\(k\)</span>…</li>
</ul>
</div>
<div id="section-analytical-form-prior-distribution" class="section level3">
<h3>Analytical form: Prior distribution</h3>
<ul>
<li>The <em>beta distribution</em> is a suitable candidate for characterizing our prior beliefs: <span class="math inline">\(\pi \sim \text{beta}(a,b)\)</span></li>
<li>Characterized by two shape parameters, <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span></li>
<li>Constrained support: <span class="math inline">\(\pi \in [0, 1]\)</span></li>
<li>pdf: <span class="math inline">\(p(\pi) = \frac{\pi^{a-1} (1- \pi)^{b-1}}{\text{B}(a, b)}\)</span></li>
</ul>
<p><img src="03-lec_files/figure-html/beta-1.png" width="864" style="display: block; margin: auto;" /></p>
</div>
<div id="section-analytical-form-likelihood" class="section level3">
<h3>Analytical form: Likelihood</h3>
<ul>
<li>Flipping one and the same coin <span class="math inline">\(n\)</span> times is a series of Bernoulli trials</li>
<li>The <em>binomial distribution</em> describes the corresponding data generating process: <span class="math inline">\(k \sim \text{Binomial}(n, \pi)\)</span></li>
<li>pmf: <span class="math inline">\(p(k|n, \pi) = {n \choose k} \pi^k (1-\pi)^{(n-k)}\)</span></li>
</ul>
</div>
<div id="section-analytical-form-posterior-distribution" class="section level3">
<h3>Analytical form: Posterior distribution</h3>
<p>Remember: <span class="math display">\[p(\theta | \mathbf{y}) \propto p(\theta) \times p(\mathbf{y}|\theta)\]</span></p>
<p>So what does this mean in the present example?</p>
<p><span class="math display">\[\begin{split}p(\pi|n,k) &amp; \propto p(\pi) \times p(k|n, \pi) \\
 p(\pi|n,k) &amp; \propto \frac{\pi^{a-1} (1- \pi)^{b-1}}{\text{B}(a, b)} \times {n \choose k} \pi^k (1-\pi)^{(n-k)}\end{split}\]</span></p>
<p>Note that since we use the proportional version of Bayes’ Law (i.e., we do not stipulate exact equality), we can drop any constant terms that do not involve our parameter of interest, <span class="math inline">\(\pi\)</span>:</p>
<p><span class="math display">\[\begin{split}p(\pi|n,k) &amp; \propto \pi^{a-1} (1- \pi)^{b-1} \times \pi^k (1-\pi)^{(n-k)}\end{split}\]</span> The rest, then, is easy: Following the rules of exponentiation, we add exponents for identical bases. This gives us our posterior distribution for <span class="math inline">\(\pi\)</span>:</p>
<p><span class="math display">\[\begin{split}p(\pi|n,k) &amp; \propto \pi^{a+k-1} (1- \pi)^{b+n-k-1}\end{split}\]</span> As you see, our posterior has the exact same form as our prior. It is a beta distribution with updated parameters</p>
<ul>
<li><span class="math inline">\(a^{\prime} = a+k-1\)</span></li>
<li><span class="math inline">\(b^{\prime} = b+n-k-1\)</span></li>
</ul>
<p>This property is called <em>conjugacy</em>: Prior and posterior are in the same family.</p>
<p>Now, take a moment to think about our analytical solution for the updated parameters. What does it take for the data to dominate the prior? What if the prior is weak? What if the prior is strong?</p>
</div>
<div id="section-simulation" class="section level3">
<h3>Simulation</h3>
<div id="section-prior-distribution-1" class="section level4">
<h4>Prior distribution</h4>
<details>
<p><summary> Code: Defining and plotting the prior distribution</summary></p>
<pre class="r"><code>len_pi &lt;- 1001L                      ### number of candidate values for pi
pi &lt;- seq(0, 1, length.out = len_pi) ### candidate values for pi
a &lt;- b &lt;- 5                          ### hyperparameters
prior &lt;- dbeta(pi, a, b)             ### prior distribution

## Plot
plot(                                ### set up empty plot, specify labels
  pi, prior,
  type = &#39;n&#39;,
  xlab = &quot;Density&quot;,
  ylab = expression(paste(&quot;Prior Distribution for &quot;, pi))
)
polygon(                             ### draw density distribution
  c(rep(0, length(pi)), pi),
  c(prior, rev(prior)),
  col = adjustcolor(&#39;red&#39;, alpha.f = .4),
  border = NA
)
abline(                              ### add vertical at pi = 0.5 
  v = .5,
  col = &#39;white&#39;
)</code></pre>
</details>
<p><img src="03-lec_files/figure-html/coin-sim0-print-1.png" width="75%" style="display: block; margin: auto;" /></p>
</div>
<div id="section-posterior-distribution-1" class="section level4">
<h4>Posterior distribution</h4>
<details>
<p><summary> Code: Simulating the experiment</summary></p>
<pre class="r"><code>set.seed(20210329)                   ### set seed for replicability
len_pi &lt;- 1001L                      ### number of candidate values for pi
pi &lt;- seq(0, 1, length.out = len_pi) ### candidate values for pi
a &lt;- b &lt;- 5                          ### hyperparameters
n &lt;- 200                             ### num. of coin flips
pi_true &lt;- .8                        ### true parameter
data &lt;- rbinom(n, 1, pi_true)        ### n coin flips
posterior &lt;- matrix(NA, 3L, n)       ### matrix container for posterior

for (i in seq_len(n)) {    
  current_sequenceuence &lt;- data[1:i]      ### sequence up until ith draw
  k &lt;- sum(current_sequenceuence)         ### number of heads in current sequence
  
  ##### Updating
  a_prime &lt;- a + k               
  b_prime &lt;- b + i - k
  
  ### Analytical means and credible intervals
  posterior[1, i] &lt;- a_prime / (a_prime + b_prime)
  posterior[2, i] &lt;- qbeta(0.025, a_prime, b_prime)
  posterior[3, i] &lt;- qbeta(0.975, a_prime, b_prime)
}

## Plot
plot(                                ### set up empty plot with labels
  1:n, 1:n,
  type = &#39;n&#39;,
  xlab = &quot;Number of Coin Flips&quot;,
  ylab = expression(paste(&quot;Posterior Means of &quot;,
                          pi,
                          sep = &quot; &quot;)), 
  ylim = c(0, 1),
  xlim = c(1, n)
)
abline(                              ### reference line for the true pi
  h = c(.5, .8),
  col = &quot;gray80&quot;
)
rect(-.5, qbeta(0.025, 5, 5),        ### prior mean + interval at i = 0
     0.5, qbeta(0.975, 5, 5),
     col = adjustcolor(&#39;red&#39;, .4),
     border = adjustcolor(&#39;red&#39;, .2))
segments(-.5, .5,
         0.5, .5,
         col = adjustcolor(&#39;red&#39;, .9),
         lwd = 1.5)
polygon(                             ### posterior means + intervals
  c(seq_len(n), rev(seq_len(n))),
  c(posterior[2, ], rev(posterior[3, ])),
  col = adjustcolor(&#39;blue&#39;, .4),
  border = adjustcolor(&#39;blue&#39;, .2)
)
lines(
  seq_len(n),
  posterior[1, ],
  col = adjustcolor(&#39;blue&#39;, .9),
  lwd = 1.5
)</code></pre>
</details>
<p><img src="03-lec_files/figure-html/coin-sim2-1.png" width="75%" style="display: block; margin: auto;" /></p>
</div>
</div>
</div>
<div id="section-mcmc-algorithms" class="section level2">
<h2>MCMC algorithms</h2>
<div id="section-analytical-classical-bayesian-inference" class="section level3">
<h3>Analytical (classical) Bayesian inference</h3>
<ul>
<li>As you may have noticed: Our coin flip example did <em>not</em> involve <em>any</em> numerical estimation algorithms.</li>
<li>We simply observed the data, applied Bayes’ Law, and analytically updated our parameters.</li>
<li>This allowed us to retrieve a distributional characterization of our parameter of interest at each iteration of the coin flip series.</li>
<li>The reasons why we could do this with ease is that this simple Binomial problem involved a single parameter <span class="math inline">\(\pi\)</span>; i.e, we were dealing with a uni-dimensional <em>parameter space</em>.</li>
</ul>
</div>
<div id="section-the-limits-of-analytical-bayesian-inference" class="section level3">
<h3>The limits of analytical Bayesian inference</h3>
<ul>
<li>Bayesian inference involves finding a <em>joint</em> posterior for <em>all</em> parameters in a model. We thus have a <em>multi-dimensional</em> parameter space.</li>
<li>Inference on single parameters from a joint multi-dimensional parameter space requires that we retrieve the marginal posterior distribution from the joint posterior distribution.</li>
<li>Marginalizing the joint multidimensional posterior distribution w.r.t. to a given a parameter gives the posterior distribution for that parameter. This requires <em>integrating</em> out all other parameters.</li>
<li>For instance, when our joint posterior is <span class="math inline">\(p(\alpha,\beta, \gamma)\)</span>, we need to obtain each marginal posterior akin to <span class="math inline">\(p(\alpha) = \int_{\beta} \int_{\gamma} p(\alpha,\beta, \gamma) d\beta d\gamma\)</span></li>
<li>For complex multi-dimensional posterior distributions, findings analytical solutions through integration becomes cumbersome, if not outright impossible.</li>
</ul>
</div>
<div id="section-numerical-approximation-via-mcmc" class="section level3">
<h3>Numerical approximation via MCMC</h3>
<p>That’s where numerical approximation through Markov Chain Monte Carlo (MCMC) algorithms comes in:</p>
<ul>
<li>MCMC are iterative computational processes that explore and describe a posterior distribution.</li>
<li><em>Markov Chains</em> wander through, and take samples from, the parameter space.</li>
<li>First developed in the 1980s and popularized in the 1990s, MCMC algorithms quickly eliminated the need for analytical marginalizations of single parameters from joint multi-dimensional posteriors.</li>
<li>The core idea: Following an initial warmup period, the Markov Chains will converge to high-density regions of the underlying posterior distribution (ergodicity).</li>
<li>The frequency of “steps” (iterations) in a given region of multidimensional parameter space gives a stochastic simulation of the posterior probability density.</li>
<li>This yields a numerical approximation of the underlying posterior distribution, much like Monte Carlo simulations of MLE parameters yield numerical approximations of the underlying sampling distribution.</li>
</ul>
</div>
<div id="section-some-mcmc-algorithms" class="section level3">
<h3>(Some) MCMC Algorithms</h3>
<ol style="list-style-type: decimal">
<li><strong>Gibbs</strong>: Draws iteratively and alternatively from the conditional conjugate distribution of each parameter.</li>
<li><strong>Metropolis-Hastings</strong>: Considers a single multidimensional move on each iteration depending on the quality of the proposed candidate draw.</li>
<li><strong>Hamiltonian Monte Carlo (HMC)</strong>, used in Stan:</li>
</ol>
<blockquote>
<sub> The Hamiltonian Monte Carlo algorithm starts at a specified initial set of parameters <span class="math inline">\(\theta\)</span>; in Stan, this value is either user-specified or generated randomly. Then, for a given number of iterations, a new momentum vector is sampled and the current value of the parameter <span class="math inline">\(\theta\)</span> is updated using the leapfrog integrator with discretization time <span class="math inline">\(\epsilon\)</span> and number of steps <span class="math inline">\(L\)</span> according to the Hamiltonian dynamics. Then a Metropolis acceptance step is applied, and a decision is made whether to update to the new state <span class="math inline">\((\theta^{\ast},\rho{\ast})\)</span> or keep the existing state. </sub>
</blockquote>
<div style="text-align: right">
<p><sub><sup> Source: <a href="https://mc-stan.org/docs/2_19/reference-manual/hamiltonian-monte-carlo.html">Stan Reference Manual, Section 14.1</a> </sub></sup></p>
</div>
</div>
</div>
<div id="section-implementing-a-gibbs-sampler" class="section level2">
<h2>Implementing a Gibbs sampler</h2>
<div id="section-roadmap" class="section level3">
<h3>Roadmap</h3>
<ul>
<li>We will now apply an MCMC algorithm to a simple two-dimensional problem: Inference on the mean and variance parameters of the normal distribution.</li>
<li>We will use a <em>Gibbs sampler</em>. Remember that Gibbs draws iteratively and alternatively from the conditional conjugate distribution of each parameter.</li>
<li>We thus need some analytical preliminaries: Namely, analytical forms for the posterior distributions of the two parameters from whose marginal posteriors we would like to sample.</li>
<li>Note that this does <em>not</em> involve marginalizing out the “unwanted” parameters; instead, we express the posterior as conditional (i.e., joint) functions of <span class="math inline">\(\mu\)</span> and <span class="math inline">\(\sigma^2\)</span>:
<ul>
<li><span class="math inline">\(p(\mu | \theta, \omega, \sigma, \mathbf{y})\)</span></li>
<li><span class="math inline">\(p(\sigma | \alpha, \beta, \mu, \mathbf{y})\)</span></li>
</ul></li>
</ul>
</div>
<div id="section-in-a-nutshell" class="section level3">
<h3>In a nutshell</h3>
<ul>
<li>We want to perform inference on a variable <span class="math inline">\(\mathbf{y}\)</span>, of which we have <span class="math inline">\(N\)</span> observations.</li>
<li>We stipulate that the data-generating process that produces <span class="math inline">\(\mathbf{y}\)</span> is normal: <span class="math inline">\(\mathbf{y} \sim \text{N}(\mu, \sigma^2)\)</span>.</li>
<li>For reasons of convenience, we parameterize the variance of this normal distribution in terms of its precision <span class="math inline">\(\tau = \frac{1}{\sigma^2}\)</span>, not in terms of its standard deviation or variance.</li>
</ul>
</div>
<div id="section-application" class="section level3">
<h3>Application</h3>
<ul>
<li>Specifically, we will focus on the variable <code>sup_afd</code> from the data set <code>gles</code>.</li>
<li>Let’s pretend our prior belief is completely naive:
<ul>
<li>We don’t know how (un)popular the AfD is in the German electorate</li>
<li>But we know that individual support is measured on a -5 to 5 scale</li>
<li>Our prior belief for <span class="math inline">\(\mu\)</span> should thus be agnostic as to whether people like or dislike the AfD, but sufficiently vague to allow for the possibility that we may be wrong: <span class="math inline">\(\mu \sim \text{N}(\theta = 0, \sigma^2 = 10)\)</span></li>
<li>Our prior belief for <span class="math inline">\(\sigma^2 = \frac{1}{\tau}\)</span> will also be vague: <span class="math inline">\(\tau \sim \Gamma(\alpha = 20, \beta = 200)\)</span></li>
<li>We have no prior belief about the dependence of both parameters and hence specify independent prior distributions</li>
</ul></li>
</ul>
</div>
<div id="section-analytical-preliminaries-mu" class="section level3">
<h3>Analytical preliminaries: <span class="math inline">\(\mu\)</span></h3>
<p>Our prior belief that <span class="math inline">\(\mu\)</span> is distributed normal with mean <span class="math inline">\(\theta = 0\)</span> and precision <span class="math inline">\(\omega = .1\)</span>, i.e., <span class="math inline">\(\mu \sim \text{N}(0, 10)\)</span>. Hence, the prior pdf is given by:</p>
<p><span class="math display">\[
p(\mu | \theta, \omega) = \sqrt{\frac{\omega}{2 \pi}} \exp \left (-\frac{\omega (\mu - \theta)^2}{2} \right)
\]</span></p>
<p>while the likelihood for the data <span class="math inline">\(\mathbf{y}\)</span> is given by</p>
<p><span class="math display">\[
p(\mathbf{y} | \mu, \tau) = \prod_{i}^{N} \sqrt{\frac{\tau}{2\pi}} \exp\left(-\frac{\tau(y_i-\mu)^2}{2} \right)
\]</span></p>
<p>Multiplying prior and likelihood and performing some algebraic transformations, we see that our conditional posterior density will be</p>
<p><span class="math display">\[
p(\mu | \theta, \omega, \tau, \mathbf{y}) \propto \exp \left(-\frac{\omega + N \tau}{2}  \left(\mu - \frac{\omega \theta + N \tau \bar{y}}{\omega + N \tau}\right)^2 \right)
\]</span></p>
<p>We recognize this as the normal pdf with mean <span class="math inline">\(\theta^{\ast} = \frac{\omega \theta + N \tau \bar{y}}{\omega + N \tau}\)</span> and precision <span class="math inline">\(\omega^{\ast} = \omega + N \tau\)</span>.</p>
<p>This gives us the required analytical solutions for the normal parameters that characterize the posterior density of <span class="math inline">\(\mu\)</span> that we want our Gibbs sampler to explore.</p>
</div>
<div id="section-analytical-preliminaries-tau" class="section level3">
<h3>Analytical preliminaries: <span class="math inline">\(\tau\)</span></h3>
<p>Furthermore, for our prior knowledge about the precision, we assume that <span class="math inline">\(\tau\)</span> is Gamma-distributed with shape <span class="math inline">\(\alpha=20\)</span> and rate <span class="math inline">\(\beta = 200\)</span>: <span class="math inline">\(\tau \sim \Gamma(20, 200)\)</span> which yields the prior pdf:</p>
<p><span class="math display">\[
p(\tau | \alpha, \beta) =  \frac{\beta^{\alpha}}{\Gamma(\alpha)} \tau^{\alpha - 1} \exp(-\beta \tau)
\]</span></p>
<p>while the likelihood for the data is still given by</p>
<p><span class="math display">\[
p(\mathbf{y} | \mu, \tau) = \prod_{i}^{N} \sqrt{\frac{\tau}{2\pi}} \exp\left(-\frac{\tau(y_i-\mu)^2}{2}\right)
\]</span></p>
<p>Once again taking the product and rearranging, we find that the conditional posterior pdf of <span class="math inline">\(\tau\)</span> is given by</p>
<p><span class="math display">\[
p(\tau | \alpha, \beta, \mu, \mathbf{y}) \propto \tau^{\alpha + \frac{N}{2} - 1} \exp\left(-\left(\beta + \sum_{i=1}^{N} \frac{(y_i - \mu)^2}{2} \tau\right)\right)
\]</span></p>
<p>This is a gamma distribution with updated parameters <span class="math inline">\(\alpha^{\ast} = \alpha + \frac{N}{2}\)</span> and <span class="math inline">\(\beta^{\ast} = \beta + \sum_{i=1}^{N} \frac{(x_i - \mu)^2}{2}\)</span>. Thus, we also have analytical solutions for the Gamma parameters that characterize the posterior density of <span class="math inline">\(\tau\)</span>.</p>
</div>
<div id="section-simulating-the-independent-prior-distributions" class="section level3">
<h3>Simulating the independent prior distributions</h3>
<details>
<p><summary> Code: Function for simulating the priors</summary></p>
<pre class="r"><code># Function
draw_from_prior &lt;-
  function(theta,
           omega,
           alpha,
           beta,
           n_draws,
           seed = 20210329) {
    # Set seed
    set.seed(seed)
    
    # Take draws
    mu &lt;- rnorm(n_draws, theta, 1 / sqrt(omega))
    tau &lt;- rgamma(n_draws, alpha, beta)
    
    ## Return output
    return(list(mu = mu,
                tau = tau))
  }</code></pre>
</details>
<div class="tutorial-exercise" data-label="prior-sim" data-caption="Code" data-completion="1" data-diagnostics="1" data-startover="1" data-lines="20">
<pre class="text"><code># Apply function
draws_prior &lt;-
  draw_from_prior(
    theta = 0,
    omega = .1,
    alpha = 20,
    beta = 200,
    n_draws = 4000
  )

# Plots of Marginal Densities
par(mfrow = c(1, 3), oma = c(0, 0, 3, 0))
plot(density(draws_prior$mu),
     main = expression(&quot;Marginal Density of&quot; ~ mu))
plot(density(draws_prior$tau),
     main = expression(&quot;Marginal Density of&quot; ~ tau))
plot(density(1 / draws_prior$tau),
     main = expression(&quot;Marginal Density of&quot; ~ sigma^2))
title(&quot;Prior Distribution of Mean and Precision&quot;, outer = T)</code></pre>
<script type="application/json" data-opts-chunk="1">{"fig.width":9,"fig.height":5,"fig.retina":2,"fig.align":"center","fig.keep":"high","fig.show":"asis","out.width":864,"warning":true,"error":false,"message":true,"exercise.df_print":"paged","exercise.checker":"NULL"}</script>
</div>
</div>
<div id="section-implementing-the-gibbs-sampler-for-the-posterior" class="section level3">
<h3>Implementing the Gibbs sampler for the posterior</h3>
<details>
<p><summary> Code: Gibbs sampler for the posterior</summary></p>
<pre class="r"><code># Define function
draw_from_posterior &lt;- function(theta,
                                omega,
                                alpha,
                                beta,
                                n_warmup,
                                n_draws,
                                data,
                                seed = 20210329,
                                keep_warmup = TRUE) {
  # Set seed
  set.seed(seed)

  # Length of chain
  len_chain &lt;- n_warmup + n_draws
  
  # Data characteristics
  n_data &lt;- length(data)  
  mean_data &lt;- mean(data) 

  # Initialize containers
  mu &lt;- rep(NA, len_chain)
  tau &lt;- rep(NA, len_chain)
  
  # Run Gibbs sampler
  for (i in seq_len(len_chain)) {
    if (i == 1) {
      ## Iteration 1: Initialize from prior
      alpha_star &lt;- alpha
      beta_star &lt;- beta
    } else {
      ## Iterations 2+: Update alpha and beta
      alpha_star &lt;- alpha + n_data / 2
      beta_star &lt;- beta + sum(((data - mu[i - 1]) ^ 2) / 2)
    }
    
    ## Sample tau
    tau[i] &lt;- rgamma(1, alpha_star, beta_star)
    
    ## Update theta and omega
    theta_star &lt;-
      (omega * theta + n_data * tau[i] * mean_data) /
      (omega + n_data * tau[i])
    omega_star &lt;- omega + n_data * tau[i]
    
    ## Sample mu
    mu[i] &lt;- rnorm(1, theta_star, 1 / sqrt(omega_star))
  }
  
  ## Conditionally discard warmup-draws
  if (!keep_warmup) {
    tau &lt;- tau[(n_warmup + 1):len_chain]
    mu &lt;- mu[(n_warmup + 1):len_chain]
  }
  
  ## Return output
  return(list(mu = mu,
              tau = tau))
}</code></pre>
</details>
<div class="tutorial-exercise" data-label="posterior-sim" data-caption="Code" data-completion="1" data-diagnostics="1" data-startover="1" data-lines="30">
<pre class="text"><code># Apply function
draws_posterior &lt;-
  draw_from_posterior(
    theta = 0,
    omega = .1,
    alpha = 20,
    beta = 200,
    n_warmup = 1000,
    n_draws = 1000,
    data = gles$sup_afd,
    keep_warmup = TRUE
  )

draws_posterior_post_warmup &lt;-
  draw_from_posterior(
    theta = 0,
    omega = .1,
    alpha = 20,
    beta = 200,
    n_warmup = 1000,
    n_draws = 1000,
    data = gles$sup_afd,
    keep_warmup = FALSE
  )

# Plots of Marginal Densities and Trace Plots
par(mfrow = c(3, 2),
    oma = c(0, 0, 3, 0))

# Plot  mu
plot(
  seq_along(draws_posterior$mu),
  draws_posterior$mu,
  type = &quot;l&quot;,
  lwd = .3,
  main = expression(&quot;Trace Plot for&quot; ~ mu),
  xlab = &quot;Draws&quot;,
  ylab = expression(mu)
)
lines(predict(loess(
  draws_posterior$mu ~ seq_along(draws_posterior$mu), span = .1
)),
col = &#39;red&#39;, lwd = 1)
plot(density(draws_posterior_post_warmup$mu),
     main = expression(&quot;Marginal Density of&quot; ~ mu))

# Plot tau
plot(
  seq_along(draws_posterior$tau),
  draws_posterior$tau,
  type = &quot;l&quot;,
  lwd = .3,
  main = expression(&quot;Trace Plot for&quot; ~ tau),
  xlab = &quot;Draws&quot;,
  ylab = expression(tau)
)
lines(predict(loess(
  draws_posterior$tau ~ seq_along(draws_posterior$tau), span = .1
)),
col = &#39;red&#39;, lwd = 1)
plot(density(draws_posterior_post_warmup$tau),
     main = expression(&quot;Marginal Density of&quot; ~ tau))
title(&quot;Posterior Distribution of Mean and Precision&quot;, outer = T)

# Plot sigma^2
plot(
  seq_along(draws_posterior$tau),
  1 / draws_posterior$tau,
  type = &quot;l&quot;,
  lwd = .3,
  main = expression(&quot;Trace Plot for&quot; ~ sigma ^ 2),
  xlab = &quot;Draws&quot;,
  ylab = expression(tau)
)
lines(predict(loess(
  I(1 / draws_posterior$tau) ~ seq_along(draws_posterior$tau), span = .1)),
col = &#39;red&#39;, lwd = 1)
plot(density(1 / draws_posterior_post_warmup$tau),
     main = expression(&quot;Marginal Density of&quot; ~ sigma ^ 2))
title(&quot;Posterior Distribution of Mean and Precision&quot;, outer = T)</code></pre>
<script type="application/json" data-opts-chunk="1">{"fig.width":9,"fig.height":9,"fig.retina":2,"fig.align":"center","fig.keep":"high","fig.show":"asis","out.width":864,"warning":true,"error":false,"message":true,"exercise.df_print":"paged","exercise.checker":"NULL"}</script>
</div>
</div>
<div id="section-how-the-sampler-explores-the-joint-posterior-density" class="section level3">
<h3>How the sampler explores the joint posterior density</h3>
<div class="tutorial-exercise" data-label="joint-posterior" data-caption="Code" data-completion="1" data-diagnostics="1" data-startover="1" data-lines="30">
<pre class="text"><code># Apply function
draws_posterior &lt;-
  draw_from_posterior(
    theta = 0,
    omega = .1,
    alpha = 20,
    beta = 200,
    n_warmup = 1000,
    n_draws = 1000,
    data = gles$sup_afd,
    keep_warmup = TRUE
  )

# Save as matrix, transform precision to variance
draws_posterior &lt;- simplify2array(draws_posterior)
draws_posterior[, 2] &lt;- 1 / draws_posterior[, 2]

# Plot

for (i in 1:nrow(draws_posterior)) {
  if (i == 1) {
    plot(
      draws_posterior[i, 1],
      draws_posterior[i, 2],
      pch = 19,
      xlim = range(draws_posterior[, 1]),
      ylim = range(draws_posterior[, 2]),
      main = &quot;Exploration of the joint posterior&quot;,
      xlab = expression(mu),
      ylab = expression(sigma),
      col = &quot;gray60&quot;
    )
  } else {
    segments(draws_posterior[i - 1, 1],
             draws_posterior[i - 1, 2],
             draws_posterior[i, 1],
             draws_posterior[i, 2],
             col = adjustcolor(&quot;gray10&quot;, alpha.f = 0.075),
             lwd = 3)
  }
}</code></pre>
<script type="application/json" data-opts-chunk="1">{"fig.width":6,"fig.height":6,"fig.retina":2,"fig.align":"center","fig.keep":"high","fig.show":"asis","out.width":576,"warning":true,"error":false,"message":true,"exercise.df_print":"paged","exercise.checker":"NULL"}</script>
</div>
</div>
</div>
<div id="section-convergence-diagnostics" class="section level2">
<h2>Convergence diagnostics</h2>
<div id="section-why-diagnose" class="section level3">
<h3>Why diagnose?</h3>
<p>MCMC algorithms use iterative algorithms to explore posterior distributions and to produce numerical approximations thereof.</p>
<p>However, even with appropriately specified models and algorithms, we can never know a priori if and when a chain has converged to its target distribution. We must thus rely on <em>convergence diagnostics</em>.</p>
<p><em>Important:</em> Convergence diagnostics cannot show or prove convergence. They can only show signs of non-convergence!</p>
<p>To be conclude that the post-warmup draws of our sampler in fact explore the target distribution, we want to show at least two things:</p>
<ol style="list-style-type: decimal">
<li>Every chain is in a stationary state (i.e., does not “wander off” the target distribution)</li>
<li>Multiple independent chains are in the same stationary state (i.e., no convergence to different target distributions given identical data)</li>
</ol>
</div>
<div id="section-generic-diagnostics" class="section level3">
<h3>Generic diagnostics</h3>
<p>Generic diagnostics (see <a href="https://www.routledge.com/Bayesian-Methods-A-Social-and-Behavioral-Sciences-Approach-Third-Edition/Gill/p/book/9781439862483">Gill 2015, Ch. 14.3</a>) include:</p>
<ol style="list-style-type: decimal">
<li><strong>Potential scale reduction statistic</strong> <span class="math inline">\(\hat{R}\)</span> (aka Gelman-Rubin convergence diagnostic) <span class="math display">\[\small \widehat{Var}(\theta) = (1 - \frac{1}{\mathtt{n_{iter}}})
 \underbrace{\Bigg(\frac{1}{ \mathtt{n_{chains}} (\mathtt{n_{iter}} - 1)} \sum_{j=1}^{\mathtt{n_{chains}}} \sum_{i=1}^{\mathtt{n_{iter}}} (\theta_{ij} - \bar{\theta_j})^2 \Bigg)}_{\text{Within chain var}} + 
 \frac{1}{\mathtt{n_{iter}}}  \underbrace{\Bigg(\frac{\mathtt{n_{iter}}}{\mathtt{n_{chains} - 1}} \sum_{j=1}^{\mathtt{n_{chains}}} (\bar{\theta_j} - \bar{\bar{\theta}})^2\Bigg)}_{\text{Between chain var}}\]</span>
<ul>
<li>low values indicate that chains are stationary (convergence to target distribution within chains)</li>
<li>low values indicate that chains mix (convergence to same target distribution across chains)</li>
</ul></li>
<li><strong>Geweke Time-Series Diagnostic</strong>: Compare non-overlapping post-warmup portions of each chain to test within-convergence</li>
<li><strong>Heidelberger and Welch Diagnostic</strong>: Compare early post-warmup portion of each chain with late portion to test within-convergence</li>
<li><strong>Raftery and Lewis Integrated Diagnostic</strong>: Evaluates the full chain of a pilot run (requires that <code>save_warmup = TRUE</code>) to estimate minimum required length of warmup and sampling</li>
</ol>
<p>These are implemented as part of the <code>coda</code> package (Output Analysis and Diagnostics for MCMC).</p>
</div>
<div id="section-visual-diagnostics" class="section level3">
<h3>Visual diagnostics</h3>
<p>The most widespread visual diagnostics are:</p>
<ol style="list-style-type: decimal">
<li><strong>Traceplots</strong>: Visually inspect if chains are stationary and have converged to the same distribution</li>
<li><strong>Autocorrelation plots</strong>: Visually inspect if the chain is sluggish in exploring the parameter space.</li>
</ol>
</div>
<div id="section-application-1" class="section level3">
<h3>Application</h3>
<p>In the following, we will use multiple chain runs of our sampler in conjunction with the <code>coda</code> package to check for signs of non-convergence.</p>
<p>Note that <code>coda</code> functions require that we combine our chains into <code>mcmc.list</code> objects.</p>
</div>
<div id="section-raftery-and-lewis-integrated-diagnostic" class="section level3">
<h3>Raftery and Lewis Integrated Diagnostic</h3>
<p>The Raftery-Lewis diagnostic takes a single chain, including warm-up draws, to estimate the minimum required length of warmup and sampling runs:</p>
<div class="tutorial-exercise" data-label="raftery-lewis" data-caption="Code" data-completion="1" data-diagnostics="1" data-startover="1" data-lines="0">
<pre class="text"><code># Example: Gill 2015, p. 503
# If we want a 95% credible interval around the median 
# with reliability between 92.5% and 97.5%, we need:
q &lt;- 0.5    # quantile of interest
r &lt;- 0.0125 # margin of error
s &lt;- 0.95   # desired reliability

## The recommend length for the pilot run:
n &lt;- ceiling((qnorm(.5 * (s + 1)) * sqrt(q * (1 - q)) / r) ^ 2)

# Pilot run
draws_pilot &lt;-
  draw_from_posterior(
    theta = 0,
    omega = .1,
    alpha = 20,
    beta = 200,
    n_warmup = 0,
    n_draws = n,
    data = gles$sup_afd,
    keep_warmup = TRUE
  )

# Save as mcmc
draws_pilot &lt;- as.mcmc(simplify2array(draws_pilot))

# Diagnose
raftery.diag(
  draws_pilot,
  q = q,
  r = r,
  s = s
)</code></pre>
<script type="application/json" data-opts-chunk="1">{"fig.width":6.5,"fig.height":4,"fig.retina":2,"fig.align":"default","fig.keep":"high","fig.show":"asis","out.width":624,"warning":true,"error":false,"message":true,"exercise.df_print":"paged","exercise.checker":"NULL"}</script>
</div>
</div>
<div id="section-gelman-rubin-geweke-and-heidelberger-welch-diagnostics" class="section level3">
<h3>Gelman-Rubin, Geweke, and Heidelberger-Welch diagnostics</h3>
<p>We will use the recommended run-length from the Raftery-Lewis diagnostic for four independent runs of our sampler.</p>
<p>We will ensure that ou chains run independently (i.e., using different starting values and different random number sequences) by setting different seed:</p>
<pre class="r"><code>seeds &lt;- sample(10001:99999, 4)
draws_multiple_chains &lt;- lapply(seeds,
                                function(seed) {
                                  as.mcmc(simplify2array(
                                    draw_from_posterior(
                                      theta = 0,
                                      omega = .1,
                                      alpha = 20,
                                      beta = 200,
                                      n_warmup = 200,
                                      n_draws = 6147,
                                      data = gles$sup_afd,
                                      keep_warmup = FALSE,
                                      seed = seed
                                    )
                                  ))
                                })

# Save as mcmc.list
draws_multiple_chains &lt;- as.mcmc.list(draws_multiple_chains)</code></pre>
<div class="tutorial-exercise" data-label="other-diagnostics" data-caption="Code" data-completion="1" data-diagnostics="1" data-startover="1" data-lines="0">
<pre class="text"><code># Diagnose
coda::gelman.diag(draws_multiple_chains, autoburnin = FALSE)
coda::geweke.diag(draws_multiple_chains, frac1 = .1, frac2 = .5)  
coda::heidel.diag(draws_multiple_chains, pvalue = .1)             </code></pre>
<script type="application/json" data-opts-chunk="1">{"fig.width":6.5,"fig.height":4,"fig.retina":2,"fig.align":"default","fig.keep":"high","fig.show":"asis","out.width":624,"warning":true,"error":false,"message":true,"exercise.df_print":"paged","exercise.checker":"NULL"}</script>
</div>
</div>
<div id="section-trace-plots" class="section level3">
<h3>Trace plots</h3>
<div class="tutorial-exercise" data-label="trace" data-caption="Code" data-completion="1" data-diagnostics="1" data-startover="1" data-lines="0">
<pre class="text"><code>par(mfrow = c(1, 2))
coda::traceplot(draws_multiple_chains, smooth = TRUE)</code></pre>
<script type="application/json" data-opts-chunk="1">{"fig.width":6.5,"fig.height":4,"fig.retina":2,"fig.align":"default","fig.keep":"high","fig.show":"asis","out.width":624,"warning":true,"error":false,"message":true,"exercise.df_print":"paged","exercise.checker":"NULL"}</script>
</div>
</div>
<div id="section-autocorrelation-plots" class="section level3">
<h3>Autocorrelation plots</h3>
<div class="tutorial-exercise" data-label="autocorr" data-caption="Code" data-completion="1" data-diagnostics="1" data-startover="1" data-lines="0">
<pre class="text"><code>coda::autocorr.plot(draws_multiple_chains)</code></pre>
<script type="application/json" data-opts-chunk="1">{"fig.width":6.5,"fig.height":4,"fig.retina":2,"fig.align":"default","fig.keep":"high","fig.show":"asis","out.width":624,"warning":true,"error":false,"message":true,"exercise.df_print":"paged","exercise.checker":"NULL"}</script>
</div>
</div>
</div>
<div id="section-contrasting-bayesian-and-frequentist-approaches" class="section level2">
<h2>Contrasting Bayesian and frequentist approaches</h2>
<div id="section-priors" class="section level3">
<h3>Priors</h3>
<ul>
<li>Choice of priors allows us to explicitly incorporate prior beliefs about parameters…</li>
<li>…but also comes with the obligation to be transparent and responsible with respect to the subjectivity this brings into our analyses</li>
</ul>
</div>
<div id="section-inference" class="section level3">
<h3>Inference</h3>
<div id="section-interpretation" class="section level4">
<h4>Interpretation</h4>
<ul>
<li>Bayesian inference does note presume large (quasi-infinite) streams of independent identically distributed (IID) data; data are considered fixed, parameters random.</li>
<li>This allows for straightforward interpretations of inferential uncertainty:
<ul>
<li><em>Bayesian</em>: “Given the data, we can conclude that there is a 95% probability that the mean is between 8 and 12, with highest probability density at a value of 10”.</li>
<li><em>Frequentist</em>: “If we took a large number of independent random samples from the same population and constructed a 95% confidence interval around the sample for each of them, these confidence intervals would contain the <em>true</em> population mean 95% of the time. Given this long-run frequency, we are 95% confident that the specific 95% confidence intervals from our singular sample contains the true population parameter” (yeah, right…).</li>
</ul></li>
</ul>
</div>
<div id="section-finite-sample-and-asymptotic-properties" class="section level4">
<h4>Finite-sample and asymptotic properties</h4>
<ul>
<li>Bayesian inference allows for exact inference in finite-sample applications where the asymptotic properties of MLE estimators (normal approximation, etc.)…</li>
<li>…but posterior distribution do often asymptotically converge to the sampling distribution of MLE estimators (<a href="https://en.wikipedia.org/wiki/Bernstein%E2%80%93von_Mises_theorem">Bernstein-von-Mises Theorem</a>)</li>
</ul>
</div>
</div>
<div id="section-flexibility-and-computational-reliability" class="section level3">
<h3>Flexibility and computational reliability</h3>
<ul>
<li>The use of MCMC algorithms for probabilistic approximate inference makes Bayesian approaches incredibly flexible and allows for computationally reliable estimation of complex, analytically intractable marginal likelihoods (avoids integration of super high-dimensional integrals)…</li>
<li>…but also comes with the necessity of high computational resources, long computation times, and extensive convergence diagnosis and model checking</li>
</ul>

<script type="application/shiny-prerendered" data-context="server-start">
## --- learnr ---
if ("learnr" %in% (.packages()))
  detach(package:learnr, unload = TRUE)
library(learnr)
knitr::opts_chunk$set(echo = FALSE)

## ---- CRAN Packages ----
## Save package names as a vector of strings
pkgs <-  c("coda", "foreign")

## Install uninstalled packages
lapply(pkgs[!(pkgs %in% installed.packages())], 
       install.packages,
       repos='http://cran.us.r-project.org')

## Load all packages to library and adjust options
lapply(pkgs, library, character.only = TRUE)

## ---- GitHub Packages ----


## ---- Global learnr Objects ----
gles <- 
  read.dta("https://github.com/denis-cohen/statmodeling/raw/main/data/gles.dta")

draw_from_prior <-
  function(theta,
           omega,
           alpha,
           beta,
           n_draws,
           seed = 20210329) {
    # Set seed
    set.seed(seed)
    
    # Take draws
    mu <- rnorm(n_draws, theta, 1 / sqrt(omega))
    tau <- rgamma(n_draws, alpha, beta)
    
    ## Return output
    return(list(mu = mu,
                tau = tau))
  }

draw_from_posterior <- function(theta,
                                omega,
                                alpha,
                                beta,
                                n_warmup,
                                n_draws,
                                data,
                                seed = 20210329,
                                keep_warmup = TRUE) {
  # Set seed
  set.seed(seed)

  # Length of chain
  len_chain <- n_warmup + n_draws
  
  # Data characteristics
  n_data <- length(data)  
  mean_data <- mean(data) 

  # Initialize containers
  mu <- rep(NA, len_chain)
  tau <- rep(NA, len_chain)
  
  # Run Gibbs sampler
  for (i in seq_len(len_chain)) {
    if (i == 1) {
      ## Iteration 1: Initialize from prior
      alpha_star <- alpha
      beta_star <- beta
    } else {
      ## Iterations 2+: Update alpha and beta
      alpha_star <- alpha + n_data / 2
      beta_star <- beta + sum(((data - mu[i - 1]) ^ 2) / 2)
    }
    
    ## Sample tau
    tau[i] <- rgamma(1, alpha_star, beta_star)
    
    ## Update theta and omega
    theta_star <-
      (omega * theta + n_data * tau[i] * mean_data) /
      (omega + n_data * tau[i])
    omega_star <- omega + n_data * tau[i]
    
    ## Sample mu
    mu[i] <- rnorm(1, theta_star, 1 / sqrt(omega_star))
  }
  
  ## Conditionally discard warmup-draws
  if (!keep_warmup) {
    tau <- tau[(n_warmup + 1):len_chain]
    mu <- mu[(n_warmup + 1):len_chain]
  }
  
  ## Return output
  return(list(mu = mu,
              tau = tau))
}

seeds <- sample(10001:99999, 4)
draws_multiple_chains <- lapply(seeds,
                                function(seed) {
                                  as.mcmc(simplify2array(
                                    draw_from_posterior(
                                      theta = 0,
                                      omega = .1,
                                      alpha = 20,
                                      beta = 200,
                                      n_warmup = 200,
                                      n_draws = 6147,
                                      data = gles$sup_afd,
                                      keep_warmup = FALSE,
                                      seed = seed
                                    )
                                  ))
                                })

# Save as mcmc.list
draws_multiple_chains <- as.mcmc.list(draws_multiple_chains)
</script>
 
<script type="application/shiny-prerendered" data-context="server">
learnr:::register_http_handlers(session, metadata = NULL)
</script>
 
<script type="application/shiny-prerendered" data-context="server">
session$onSessionEnded(function() {
        learnr:::session_stop_event(session)
      })
</script>
 
<script type="application/shiny-prerendered" data-context="server">
`tutorial-exercise-prior-sim-result` <- learnr:::setup_exercise_handler(reactive(req(input$`tutorial-exercise-prior-sim-code-editor`)), session)
output$`tutorial-exercise-prior-sim-output` <- renderUI({
  `tutorial-exercise-prior-sim-result`()
})
</script>
 
<script type="application/shiny-prerendered" data-context="server">
`tutorial-exercise-posterior-sim-result` <- learnr:::setup_exercise_handler(reactive(req(input$`tutorial-exercise-posterior-sim-code-editor`)), session)
output$`tutorial-exercise-posterior-sim-output` <- renderUI({
  `tutorial-exercise-posterior-sim-result`()
})
</script>
 
<script type="application/shiny-prerendered" data-context="server">
`tutorial-exercise-joint-posterior-result` <- learnr:::setup_exercise_handler(reactive(req(input$`tutorial-exercise-joint-posterior-code-editor`)), session)
output$`tutorial-exercise-joint-posterior-output` <- renderUI({
  `tutorial-exercise-joint-posterior-result`()
})
</script>
 
<script type="application/shiny-prerendered" data-context="server">
`tutorial-exercise-raftery-lewis-result` <- learnr:::setup_exercise_handler(reactive(req(input$`tutorial-exercise-raftery-lewis-code-editor`)), session)
output$`tutorial-exercise-raftery-lewis-output` <- renderUI({
  `tutorial-exercise-raftery-lewis-result`()
})
</script>
 
<script type="application/shiny-prerendered" data-context="server">
`tutorial-exercise-other-diagnostics-result` <- learnr:::setup_exercise_handler(reactive(req(input$`tutorial-exercise-other-diagnostics-code-editor`)), session)
output$`tutorial-exercise-other-diagnostics-output` <- renderUI({
  `tutorial-exercise-other-diagnostics-result`()
})
</script>
 
<script type="application/shiny-prerendered" data-context="server">
`tutorial-exercise-trace-result` <- learnr:::setup_exercise_handler(reactive(req(input$`tutorial-exercise-trace-code-editor`)), session)
output$`tutorial-exercise-trace-output` <- renderUI({
  `tutorial-exercise-trace-result`()
})
</script>
 
<script type="application/shiny-prerendered" data-context="server">
`tutorial-exercise-autocorr-result` <- learnr:::setup_exercise_handler(reactive(req(input$`tutorial-exercise-autocorr-code-editor`)), session)
output$`tutorial-exercise-autocorr-output` <- renderUI({
  `tutorial-exercise-autocorr-result`()
})
</script>
 <!--html_preserve-->
<script type="application/shiny-prerendered" data-context="dependencies">
{"type":"list","attributes":{},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["jquery"]},{"type":"character","attributes":{},"value":["1.11.3"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/jquery"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["jquery.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["2.7"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["bootstrap"]},{"type":"character","attributes":{},"value":["3.3.5"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/bootstrap"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["viewport"]}},"value":[{"type":"character","attributes":{},"value":["width=device-width, initial-scale=1"]}]},{"type":"character","attributes":{},"value":["js/bootstrap.min.js","shim/html5shiv.min.js","shim/respond.min.js"]},{"type":"character","attributes":{},"value":["css/cerulean.min.css"]},{"type":"character","attributes":{},"value":["<style>h1 {font-size: 34px;}\n       h1.title {font-size: 38px;}\n       h2 {font-size: 30px;}\n       h3 {font-size: 24px;}\n       h4 {font-size: 18px;}\n       h5 {font-size: 16px;}\n       h6 {font-size: 12px;}\n       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}\n       pre:not([class]) { background-color: white }<\/style>"]},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["2.7"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["pagedtable"]},{"type":"character","attributes":{},"value":["1.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/pagedtable-1.1"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["js/pagedtable.js"]},{"type":"character","attributes":{},"value":["css/pagedtable.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["2.7"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["highlightjs"]},{"type":"character","attributes":{},"value":["9.12.0"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/highlightjs"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["highlight.js"]},{"type":"character","attributes":{},"value":["textmate.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["2.7"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["tutorial"]},{"type":"character","attributes":{},"value":["0.10.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/tutorial"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tutorial.js"]},{"type":"character","attributes":{},"value":["tutorial.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["tutorial-autocompletion"]},{"type":"character","attributes":{},"value":["0.10.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/tutorial"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tutorial-autocompletion.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["tutorial-diagnostics"]},{"type":"character","attributes":{},"value":["0.10.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/tutorial"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tutorial-diagnostics.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["tutorial-format"]},{"type":"character","attributes":{},"value":["0.10.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmarkdown/templates/tutorial/resources"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tutorial-format.js"]},{"type":"character","attributes":{},"value":["tutorial-format.css","rstudio-theme.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["jquery"]},{"type":"character","attributes":{},"value":["1.11.3"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/jquery"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["jquery.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["2.7"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["navigation"]},{"type":"character","attributes":{},"value":["1.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/navigation-1.1"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tabsets.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["2.7"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["highlightjs"]},{"type":"character","attributes":{},"value":["9.12.0"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/highlightjs"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["highlight.js"]},{"type":"character","attributes":{},"value":["default.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["2.7"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["jquery"]},{"type":"character","attributes":{},"value":["1.11.3"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/jquery"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["jquery.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["2.7"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["font-awesome"]},{"type":"character","attributes":{},"value":["5.1.0"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/fontawesome"]}]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["css/all.css","css/v4-shims.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["2.7"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["bootbox"]},{"type":"character","attributes":{},"value":["4.4.0"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/bootbox"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["bootbox.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["idb-keyvalue"]},{"type":"character","attributes":{},"value":["3.2.0"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/idb-keyval"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["idb-keyval-iife-compat.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[false]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["tutorial"]},{"type":"character","attributes":{},"value":["0.10.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/tutorial"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tutorial.js"]},{"type":"character","attributes":{},"value":["tutorial.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["tutorial-autocompletion"]},{"type":"character","attributes":{},"value":["0.10.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/tutorial"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tutorial-autocompletion.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["tutorial-diagnostics"]},{"type":"character","attributes":{},"value":["0.10.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/tutorial"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tutorial-diagnostics.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["ace"]},{"type":"character","attributes":{},"value":["1.2.6"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/ace"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["ace.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["clipboardjs"]},{"type":"character","attributes":{},"value":["1.5.15"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/clipboardjs"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["clipboard.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["ace"]},{"type":"character","attributes":{},"value":["1.2.6"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/ace"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["ace.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["clipboardjs"]},{"type":"character","attributes":{},"value":["1.5.15"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/clipboardjs"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["clipboard.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["ace"]},{"type":"character","attributes":{},"value":["1.2.6"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/ace"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["ace.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["clipboardjs"]},{"type":"character","attributes":{},"value":["1.5.15"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/clipboardjs"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["clipboard.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["ace"]},{"type":"character","attributes":{},"value":["1.2.6"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/ace"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["ace.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["clipboardjs"]},{"type":"character","attributes":{},"value":["1.5.15"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/clipboardjs"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["clipboard.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["ace"]},{"type":"character","attributes":{},"value":["1.2.6"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/ace"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["ace.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["clipboardjs"]},{"type":"character","attributes":{},"value":["1.5.15"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/clipboardjs"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["clipboard.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["ace"]},{"type":"character","attributes":{},"value":["1.2.6"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/ace"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["ace.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["clipboardjs"]},{"type":"character","attributes":{},"value":["1.5.15"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/clipboardjs"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["clipboard.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["ace"]},{"type":"character","attributes":{},"value":["1.2.6"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/ace"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["ace.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["clipboardjs"]},{"type":"character","attributes":{},"value":["1.5.15"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/clipboardjs"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["clipboard.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]}]}
</script>
<!--/html_preserve-->
<!--html_preserve-->
<script type="application/shiny-prerendered" data-context="execution_dependencies">
{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["packages"]}},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["packages","version"]},"class":{"type":"character","attributes":{},"value":["data.frame"]},"row.names":{"type":"integer","attributes":{},"value":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,39,40,41,42,43]}},"value":[{"type":"character","attributes":{},"value":["base","bslib","coda","compiler","datasets","digest","evaluate","fastmap","foreign","graphics","grDevices","grid","highr","htmltools","htmlwidgets","httpuv","jquerylib","jsonlite","knitr","later","lattice","learnr","magrittr","markdown","methods","mime","promises","R6","Rcpp","rlang","rmarkdown","rprojroot","sass","shiny","stats","stringi","stringr","tools","utils","withr","xfun","xtable","yaml"]},{"type":"character","attributes":{},"value":["4.0.2","0.2.4","0.19-4","4.0.2","4.0.2","0.6.27","0.14","1.0.1","0.8-80","4.0.2","4.0.2","4.0.2","0.8","0.5.1.1","1.5.2","1.5.4","0.1.3","1.7.2","1.31","1.1.0.1","0.20-41","0.10.1","2.0.1","1.1","4.0.2","0.10","1.1.1","2.5.0","1.0.5","0.4.10","2.7","2.0.2","0.3.1","1.5.0","4.0.2","1.5.3","1.4.0","4.0.2","4.0.2","2.3.0","0.21","1.8-4","2.2.1"]}]}]}
</script>
<!--/html_preserve-->
</div>
</div>

</div> <!-- topics -->

<div class="topicsContainer">
<div class="topicsPositioner">
<div class="band">
<div class="bandContent topicsListContainer">

<!-- begin doc-metadata -->
<div id="doc-metadata">
<h2 class="title toc-ignore" style="display:none;">Lecture: Bayesian Fundamentals</h2>
</div>
<!-- end doc-metadata -->

</div> <!-- bandContent.topicsListContainer -->
</div> <!-- band -->
</div> <!-- topicsPositioner -->
</div> <!-- topicsContainer -->


</div> <!-- bandContent page -->
</div> <!-- pageContent band -->




<script>
// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});
</script>


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>


</body>

</html>
