<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />



<meta name="progressive" content="true" />
<meta name="allow-skip" content="true" />

<title>Lecture: Generalized Linear Models</title>


<!-- highlightjs -->
<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs) {
  hljs.configure({languages: []});
  hljs.initHighlightingOnLoad();
  if (document.readyState && document.readyState === "complete") {
    window.setTimeout(function() { hljs.initHighlighting(); }, 0);
  }
}
</script>

<!-- taken from https://github.com/rstudio/rmarkdown/blob/67b7f5fc779e4cfdfd0f021d3d7745b6b6e17149/inst/rmd/h/default.html#L296-L362 -->
<!-- tabsets -->

<style type="text/css">
.tabset-dropdown > .nav-tabs {
  display: inline-table;
  max-height: 500px;
  min-height: 44px;
  overflow-y: auto;
  background: white;
  border: 1px solid #ddd;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs > li.active:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li.active:before {
  content: "&#xe258;";
  border: none;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open:before {
  content: "";
  font-family: 'Glyphicons Halflings';
  display: inline-block;
  padding: 10px;
  border-right: 1px solid #ddd;
}

.tabset-dropdown > .nav-tabs > li.active {
  display: block;
}

.tabset-dropdown > .nav-tabs > li > a,
.tabset-dropdown > .nav-tabs > li > a:focus,
.tabset-dropdown > .nav-tabs > li > a:hover {
  border: none;
  display: inline-block;
  border-radius: 4px;
}

.tabset-dropdown > .nav-tabs.nav-tabs-open > li {
  display: block;
  float: none;
}

.tabset-dropdown > .nav-tabs > li {
  display: none;
}
</style>

<script>
$(document).ready(function () {
  window.buildTabsets("section-TOC");
});

$(document).ready(function () {
  $('.tabset-dropdown > .nav-tabs > li').click(function () {
    $(this).parent().toggleClass('nav-tabs-open')
  });
});
</script>
<!-- end tabsets -->


<link rel="stylesheet" href="css\learnr-theme.css" type="text/css" />

</head>

<body>



<div class="pageContent band">
<div class="bandContent page">

<div class="topics">

<div id="section-generalized-linear-models" class="section level2">
<h2>Generalized linear models</h2>
<div id="section-goals-for-this-session" class="section level3">
<h3>Goals for this session</h3>
<ul>
<li>To understand generalized linear models (GLM) as a unified methodology for producing parameter estimates (<a href="https://www.researchgate.net/publication/235726158_Generalized_Linear_Models_A_Unified_Approach">Gill 2001</a>, <a href="https://us.sagepub.com/en-us/nam/generalized-linear-models/book257965">Gill and Torres 2019</a>).</li>
<li>To understand that all GLM produce two important, intuitive, and substantively meaningful quantities of interest that can be derived from the parameter estimates and the data (<a href="https://gking.harvard.edu/files/gking/files/making.pdf">King et al. 2000</a>):
<ol style="list-style-type: decimal">
<li>Expected values (conditional expectations)</li>
<li>Average marginal effects or first differences</li>
</ol></li>
<li>To learn how to estimate these quantities and how to specify the uncertainty about our inferences using simulation techniques (<a href="https://gking.harvard.edu/files/gking/files/making.pdf">King et al. 2000</a>).</li>
</ul>
</div>
<div id="section-the-three-parts-of-every-glm" class="section level3">
<h3>The three parts of every GLM</h3>
<p>All generalized linear models have three characteristic parts:</p>
<div id="section-family" class="section level4">
<h4>Family</h4>
<ul>
<li>The family stipulates a stochastic process that can plausibly generate an outcome <span class="math inline">\(Y\)</span></li>
<li>This means we choose a pdf or pmf for <span class="math inline">\(y\)</span> given some parameters: <span class="math inline">\(y_i \sim \text{f}(\theta_i, \psi)\)</span></li>
<li>The choice usually depends on the distributional properties of <span class="math inline">\(Y\)</span></li>
<li>A.k.a. the data-generating process or the likelihood</li>
</ul>
</div>
<div id="section-linear-component" class="section level4">
<h4>Linear component</h4>
<ul>
<li>A linear model <span class="math inline">\(y_i^{\ast} = \mathbf{x}_i^{\prime} \beta + \epsilon\)</span></li>
<li>The goal of inference is the estimation of <span class="math inline">\(\beta\)</span></li>
<li>From, this, we can derive our <em>systematic component</em> or <em>linear predictor</em>, <span class="math inline">\(\eta_i = \mathbf{x}_i^{\prime} \beta\)</span></li>
</ul>
</div>
<div id="section-inverse-link-function" class="section level4">
<h4>Inverse link function</h4>
<ul>
<li>A function that transforms the systematic component <span class="math inline">\(\eta_i\)</span> such that it represents a characteristic <em>parameter</em> <span class="math inline">\(\theta_i\)</span> of the family</li>
<li><span class="math inline">\(\theta_i = g^{-1}(\eta_i)\)</span></li>
</ul>
</div>
</div>
<div id="section-in-a-nutshell" class="section level3">
<h3>In a nutshell</h3>
<p>Putting it all together, a GLM is given by</p>
<p><span class="math display">\[y_i \sim \text{f}(\theta_i = g^{-1}(\mathbf{x}_i^{\prime} \beta), \psi)\]</span></p>
<p>where <span class="math inline">\(\psi\)</span> is an auxiliary parameter that will sometimes be estimated (e.g., <span class="math inline">\(\sigma^2\)</span> in the linear model).</p>
<p>Every generalized linear model is a <em>special case</em> of this general framework.</p>
</div>
<div id="section-example-1-the-linear-model" class="section level3">
<h3>Example 1: The linear model</h3>
<ul>
<li>While every GLM is a special case, the linear model is arguably a <em>very</em> special case.</li>
<li>Why? Because its link function is the <em>identity function</em>.</li>
<li>This not only makes the notation easier, but also means that the <span class="math inline">\(\beta\)</span>’s are <em>directly interpretable</em> on the scale of the outcome.</li>
</ul>
<div id="section-the-three-parts" class="section level4">
<h4>The three parts:</h4>
<ul>
<li>Family: <span class="math display">\[y_i \sim \text{N}(\mu_i, \sigma^2)\]</span></li>
<li>Linear component: <span class="math display">\[y_i = \underbrace{\mathbf{x}_i^{\prime} \beta}_{\mu_i} + \underbrace{\epsilon}_{\sim \text{N}(0, \sigma^2)}\]</span></li>
<li>Inverse link function: <span class="math display">\[\mu_i = \text{id}(\mu_i)\]</span></li>
</ul>
<p>Thus, the linear model is given by</p>
<p><span class="math display">\[y_i \sim \text{N}(\mathbf{x}_i^{\prime} \beta, \sigma^2)\]</span> where both <span class="math inline">\(\beta\)</span> and <span class="math inline">\(\sigma^2\)</span> are being estimated.</p>
</div>
</div>
<div id="section-example-2-the-probit-model" class="section level3">
<h3>Example 2: The probit model</h3>
<p>The probit model is a popular choice for modeling idiosyncratic binary choices.</p>
<div id="section-the-three-parts-1" class="section level4">
<h4>The three parts:</h4>
<ul>
<li>Family: <span class="math display">\[y_i \sim \text{Bernoulli}(\pi_i)\]</span></li>
<li>Linear component: <span class="math display">\[y_i^{\ast} = \underbrace{\mathbf{x}_i^{\prime} \beta}_{\eta_i} + \underbrace{\epsilon}_{\sim \text{N}(0, 1)}\]</span></li>
<li>Inverse link function: <span class="math display">\[\pi_i = \Phi(\eta_i)\]</span></li>
</ul>
<p>Thus, the probit model is given by</p>
<p><span class="math display">\[y_i \sim \text{Bernoulli}(\Phi(\mathbf{x}_i^{\prime} \beta))\]</span></p>
<p>where the <span class="math inline">\(\beta\)</span> vector is being estimated. <span class="math inline">\(\Phi\)</span> is the standard normal CDF.</p>
</div>
</div>
<div id="section-example-3-the-logit-model" class="section level3">
<h3>Example 3: The logit model</h3>
<p>With a slight change in the error distribution in the linear component and a corresponding change in the inverse link function, we can derive the logit model for binary choices.</p>
<div id="section-the-three-parts-2" class="section level4">
<h4>The three parts:</h4>
<ul>
<li>Family: <span class="math display">\[y_i \sim \text{Bernoulli}(\pi_i)\]</span></li>
<li>Linear component: <span class="math display">\[y_i^{\ast} = \underbrace{\mathbf{x}_i^{\prime} \beta}_{\eta_i} + \underbrace{\epsilon}_{\sim \text{Logistic}(0, 1)}\]</span></li>
<li>Inverse link function: <span class="math display">\[\pi_i = \frac{\exp(\eta_i)}{1 + \exp(\eta_i)}\]</span></li>
</ul>
<p>Thus, the logit model is given by</p>
<p><span class="math display">\[y_i \sim \text{Bernoulli}\left(\frac{\exp(\eta_i)}{1 + \exp(\eta_i)}\right)\]</span></p>
<p>where the <span class="math inline">\(\beta\)</span> vector is being estimated. <span class="math inline">\(\frac{\exp(\cdot)}{1 + \exp(\cdot)}\)</span> is the standard logistic CDF. Shorthand: <span class="math inline">\(\text{logit}^{-1}(\cdot)\)</span>.</p>
<p>As you can see, the <em>assumed</em> distribution of the error term on the latent variable <span class="math inline">\(y_i^{\ast}\)</span> dictates our choice of the inverse link function.</p>
<p>As the error distribution is fixed and its parameter are not being estimated, it is, however, not in itself of substantive interest.</p>
</div>
</div>
</div>
<div id="section-glm-typology" class="section level2">
<h2>GLM Typology</h2>
<div id="section-single-family-models" class="section level3">
<h3>Single-family models</h3>
<p>We will first focus on models whose likelihood function follows a single pdf or pmf.</p>
</div>
<div id="section-univariate-eta_i-univariate-theta_i" class="section level3">
<h3>Univariate <span class="math inline">\(\eta_i\)</span>, univariate <span class="math inline">\(\theta_i\)</span></h3>
<p>Among the simplest GLM are those models that require</p>
<ul>
<li>a single family</li>
<li>a univariate systematic component <span class="math inline">\(\eta_i\)</span></li>
<li>a univariate parameter <span class="math inline">\(\theta_i\)</span></li>
</ul>
<p>Examples include the three models discussed above:</p>
<ul>
<li>The linear model (<span class="math inline">\(\eta_i = \theta_i = \mu_i\)</span>)</li>
<li>The probit model (<span class="math inline">\(\eta_i = \mathbf{x}_i^{\prime}\beta\)</span>, <span class="math inline">\(\theta_i = \Phi(\eta_i)\)</span>)</li>
<li>The logit model (<span class="math inline">\(\eta_i = \mathbf{x}_i^{\prime}\beta\)</span>, <span class="math inline">\(\theta_i = \text{logit}^{-1}(\eta_i)\)</span>)</li>
</ul>
<p>An additional example would be the <em>Poisson model</em> for counts:</p>
<ul>
<li><span class="math inline">\(y_i \sim \text{Poisson}(\exp(\theta_i = \mathbf{x}_i^{\prime}\beta))\)</span></li>
</ul>
</div>
<div id="section-univariate-eta_i-multivariate-theta_i" class="section level3">
<h3>Univariate <span class="math inline">\(\eta_i\)</span>, multivariate <span class="math inline">\(\theta_i\)</span></h3>
<p>Things get a bit more intricate when we model multivariate outcomes, e.g., discrete choice across multiple categories.</p>
<p>Multi-categorical discrete choice outcomes typically require that we stipulate a <em>categorical distribution</em> which requires choice-specific probability parameters <span class="math inline">\(\theta_{ij} = \Pr(Y_i = j)\)</span>.</p>
<p>A model that accommodates this while using a single univariate linear predictor <span class="math inline">\(\eta_i\)</span> is the <em>ordered logit model</em>, which can be used for modeling ordered outcomes with <span class="math inline">\(J\)</span> categories:</p>
<div id="section-family-1" class="section level4">
<h4>Family</h4>
<p><span class="math display">\[y_{ij} \sim \text{Categorical}(\theta_{ij})\]</span></p>
</div>
<div id="section-systematic-component" class="section level4">
<h4>Systematic component</h4>
<p>The linear predictor is <span class="math inline">\(\eta_i = \mathbf{x}_i^{\prime}\beta\)</span>, where <span class="math inline">\(\beta\)</span> does <em>not</em> include an intercept and <span class="math inline">\(\mathbf{x}_i^{\prime}\)</span> does not include a leading one.</p>
<p>In place of an intercept, the model produces <span class="math inline">\(J-1\)</span> ordered threshold parameters <span class="math inline">\(\kappa\)</span>.</p>
</div>
<div id="section-link-function" class="section level4">
<h4>Link function</h4>
<p>We then use the inverse logit link function to retrieve <span class="math inline">\(J\)</span> probabilities <span class="math inline">\(\theta_{ij}\)</span> that <span class="math inline">\(\eta_i\)</span> exceeds a given threshold <span class="math inline">\(\kappa\)</span>:</p>
<p><span class="math display">\[\begin{split}\Pr(y_i = j | \mathbf{x}_i) &amp; =\Pr(\kappa_{j-1} &lt; \eta_i \leq \kappa_j) \\ &amp; =\text{logit}^{-1} (\kappa_j - \eta_i) - \text{logit}^{-1} (\kappa_{j-1} - \eta_i)\end{split}\]</span></p>
</div>
</div>
<div id="section-multivariate-eta_i-multivariate-theta_i" class="section level3">
<h3>Multivariate <span class="math inline">\(\eta_i\)</span>, multivariate <span class="math inline">\(\theta_i\)</span></h3>
<p>When moving from ordered to unordered discrete choices, we must not only model choice-specific probability parameters <span class="math inline">\(\theta_{ij}\)</span> but also choice-specific linear predictors <span class="math inline">\(\eta_{ij}\)</span>.</p>
<p>An example is the <em>multinomial logistic regression model</em>.</p>
<div id="section-family-2" class="section level4">
<h4>Family</h4>
<p><span class="math display">\[y_{ij} \sim \text{Categorical}(\theta_{ij})\]</span></p>
</div>
<div id="section-systematic-component-1" class="section level4">
<h4>Systematic component</h4>
<p>The linear predictor is <span class="math inline">\(\eta_{ij} = \mathbf{x}_i^{\prime}\beta_j + \mathbf{z}_{ij}^{\prime} \gamma\)</span>. For statistical identification, we must set the <span class="math inline">\(\beta\)</span> vector for one category to zero, e.g., <span class="math inline">\(\beta_J = \mathbf{0}\)</span>.</p>
</div>
<div id="section-link-function-1" class="section level4">
<h4>Link function</h4>
<p>The link function is the <em>softmax</em> function, a multivariate generalization of the inverse logit function:</p>
<p><span class="math display">\[\Pr(y_i = j) = \text{softmax}(\eta_{ij})_k = \frac{\exp (\eta_{ij})}{\sum_{j=1}^{J} \exp(\eta_{ij})}\]</span></p>
</div>
</div>
<div id="section-multiple-families" class="section level3">
<h3>Multiple families</h3>
<p>Some models stipulate complex data generating processes. They combine multiple families <span class="math inline">\(f\)</span> in the likelihood (which means that the likelihood will be a mixture of the constitutive likelihoods).</p>
</div>
<div id="section-univariate-eta_i-multivariate-theta_i-1" class="section level3">
<h3>Univariate <span class="math inline">\(\eta_i\)</span>, multivariate <span class="math inline">\(\theta_i\)</span></h3>
<p>These models estimate only one set of parameters <span class="math inline">\(\beta\)</span>, but use different link functions for translating the resulting linear predictor <span class="math inline">\(\eta_i\)</span> into different parameters <span class="math inline">\(\theta_i^{f}\)</span> that match the stipulated data-generating processes <span class="math inline">\(f\)</span>.</p>
<p>A well-known case is the <em>tobit model</em> for censored data. For instance, a left-censored tobit model with a lower bound at <span class="math inline">\(y_L = 0\)</span> jointly accommodates a Bernoulli data-generating process for <span class="math inline">\(\Pr(y &gt; y_L)\)</span> and a normal data-generating process for the variation in <span class="math inline">\(y\)</span> given <span class="math inline">\(y &gt; y_L\)</span>. By assumption, both data-generating processes are governed by the same parameters <span class="math inline">\(\beta\)</span>.</p>
<p>Similar logics apply to other models that involve censored data, e.g., in survival analysis.</p>
</div>
<div id="section-multivariate-eta_i-multivariate-theta_i-1" class="section level3">
<h3>Multivariate <span class="math inline">\(\eta_i\)</span>, multivariate <span class="math inline">\(\theta_i\)</span></h3>
<div id="section-two-part-models" class="section level4">
<h4>Two-part models</h4>
<p>A generalization of this are <em>two-part models</em>. Instead of estimating one set of parameters <span class="math inline">\(\beta\)</span> and using different link functions for translating <span class="math inline">\(\eta_i\)</span> into <span class="math inline">\(\theta_i^{f}\)</span>, these models estimate distinct sets of parameters <span class="math inline">\(\beta^{f}\)</span> for each of the stipulated data-generating process.</p>
<p>An example is a <em>hurdle model</em>. Unlike a left-censored tobit model, this model allows for the possibility that different sets of parameters govern the data-generating process for <span class="math inline">\(\Pr(y &gt; y_L)\)</span> and a normal data-generating process for the variation in <span class="math inline">\(y\)</span> given <span class="math inline">\(y &gt; y_L\)</span>.</p>
</div>
<div id="section-finite-mixtures-of-identical-families" class="section level4">
<h4>Finite mixtures of identical families</h4>
<p>A different intuition underlies finite mixture models. Rather than stipulating different <em>families</em> depending on the observed values of each unit, finite mixture models stipulate that substantively different data generating processes of the <em>same family</em> may generate the observed outcomes of all observations.</p>
</div>
</div>
</div>
<div id="section-quantities-of-interest" class="section level2">
<h2>Quantities of interest</h2>
<div id="section-expected-values" class="section level3">
<h3>Expected values</h3>
<ul>
<li>The <em>expected value</em> tells you the where to expect the <em>conditional mean</em> of <span class="math inline">\(Y\)</span> given some covariate values <span class="math inline">\(\mathbf{x}\)</span> on the scale of <span class="math inline">\(Y\)</span>.</li>
<li>For most (but not all) GLM, the expected value is directly given by our estimate of the parameter <span class="math inline">\(\theta_i = g^{-1}(\mathbf{x}_i^{\prime} \beta)\)</span>:
<ul>
<li>Linear model: <span class="math inline">\(\mathbb{E}[y|\mathbf{x}] = \mathbf{x}_i^{\prime} \beta\)</span> (“predicted value”)</li>
<li>Probit model: <span class="math inline">\(\mathbb{E}[y|\mathbf{x}] = \Phi(\mathbf{x}_i^{\prime} \beta)\)</span> (“predicted probability”)</li>
<li>Logit model: <span class="math inline">\(\mathbb{E}[y|\mathbf{x}] = \text{logit}^{-1}(\mathbf{x}_i^{\prime} \beta)\)</span> (“predicted probability”)</li>
</ul></li>
</ul>
</div>
<div id="section-first-differences" class="section level3">
<h3>First differences</h3>
<ul>
<li>A first difference is the difference between two expected values.</li>
<li>It usually gives an estimate of how changing one covariate <span class="math inline">\(d\)</span> affects our conditional expectation of <span class="math inline">\(y\)</span> while holding all else (<span class="math inline">\(\mathbf{x}\)</span>) constant.
<ul>
<li>Linear model: <span class="math inline">\(\mathbb{E}[y|d_1, \mathbf{x}] - \mathbb{E}[y| d_0,\mathbf{x}] = (\alpha + \tau d_1 + \mathbf{x}_i^{\prime} \beta) - (\alpha + \tau d_0 + \mathbf{x}_i^{\prime} \beta) = \tau (d_1 - d_0)\)</span></li>
<li>Probit model: <span class="math inline">\(\mathbb{E}[y|d_1, \mathbf{x}] - \mathbb{E}[y| d_0,\mathbf{x}] = \Phi(\alpha + \tau d_1 + \mathbf{x}_i^{\prime} \beta)- \Phi(\alpha + \tau d_0 + \mathbf{x}_i^{\prime} \beta)\)</span></li>
<li>Logit model: <span class="math inline">\(\mathbb{E}[y|d_1, \mathbf{x}] - \mathbb{E}[y| d_0,\mathbf{x}] = \text{logit}^{-1}(\alpha + \tau d_1 + \mathbf{x}_i^{\prime} \beta)- \text{logit}^{-1}(\alpha + \tau d_0 + \mathbf{x}_i^{\prime} \beta)\)</span></li>
</ul></li>
</ul>
<p>An important insight is that the first difference in the linear model does <em>not</em> depend on the values of other covariates <span class="math inline">\(\mathbf{x}\)</span>.</p>
<p>In all other GLM, the presence of an inverse link function makes first differences sensitive to the choice of covariates <span class="math inline">\(\mathbf{x}\)</span>!</p>
</div>
<div id="section-marginal-effects" class="section level3">
<h3>Marginal effects</h3>
<p>The marginal effect of a variable <span class="math inline">\(d\)</span> on the expected value <span class="math inline">\(\mathbb{E}[y|d, \mathbf{x}]\)</span> is given by the marginal rate of change in <span class="math inline">\(\mathbb{E}[y|d, \mathbf{x}]\)</span> for an infinitesimal change in <span class="math inline">\(d\)</span>:</p>
<p><span class="math display">\[\frac{\mathbb{E}[y|d + \Delta_d, \mathbf{x}] - \mathbb{E}[y|d, \mathbf{x}]}{\Delta_d}\]</span></p>
<p>As <span class="math inline">\(\Delta_d \rightarrow 0\)</span>, this becomes <span class="math inline">\(\frac{\partial \mathbb{E}[y|d, \mathbf{x}]}{\partial d}\)</span>.</p>
<p>For the linear model, this is mathematically straightforward:</p>
<p><span class="math display">\[\frac{\partial (\alpha + \tau d + \mathbf{x}_i^{\prime} \beta)}{\partial d} = \tau\]</span></p>
<p>For all other GLM, we rely on <em>normalized first differences</em> for a freely chosen shift <span class="math inline">\(\Delta_d\)</span>.</p>
<p>For instance, the marginal effect in a probit model for a <em>standard deviation increase</em> in <span class="math inline">\(d\)</span> is given by</p>
<p><span class="math display">\[\frac{\mathbb{E}[y|d + \text{sd}(d), \mathbf{x}] - \mathbb{E}[y|d, \mathbf{x}]}{\text{sd}(d)} = \frac{\Phi(\alpha + \tau (d + \text{sd}(d)) + \mathbf{x}_i^{\prime} \beta) - \Phi(\alpha + \tau d + \mathbf{x}_i^{\prime} \beta)}{\text{sd}(d)}\]</span> As with first differences, the marginal effect is thus sensitive to the choice of covariates <span class="math inline">\(\mathbf{x}\)</span>!</p>
</div>
<div id="section-average-quantities-of-interest" class="section level3">
<h3>Average quantities of interest</h3>
<p>The sensitivity of first differences and marginal effects to the choice of <span class="math inline">\(\mathbf{x}\)</span> is problematic because makes these quantities of interest dependent on subjective judgment.</p>
<p>For instance, the estimates of these quantities may differ dramatically depending on whether we set all variables in <span class="math inline">\(\mathbf{x}\)</span> to their sample means, sample minimums, or sample maximums.</p>
<p>A remedy is the <em>observed values approach</em> (<a href="https://onlinelibrary.wiley.com/doi/full/10.1111/j.1540-5907.2012.00602.x">Hanmer and Kalkan 2013</a>), which involves two steps:</p>
<ol style="list-style-type: decimal">
<li>Calculate unit-specific quantities of interest at the observed values <span class="math inline">\(\mathbf{x}_i\)</span> for each observation <span class="math inline">\(i = 1,...,N\)</span></li>
<li>Average across the <span class="math inline">\(N\)</span> unit-specific quantities to obtain the <em>average</em> quantities of interest</li>
</ol>
<p>So for instance, the average first difference for a binary variable <span class="math inline">\(d\)</span> in a probit model is given by</p>
<p><span class="math display">\[\frac{1}{N} \sum_{i=1}^{N}\Phi(\alpha + \tau (1) + \mathbf{x}_i^{\prime} \beta) - \Phi(\alpha + \tau (0) + \mathbf{x}_i^{\prime} \beta)\]</span></p>
<p>Analogously, the average marginal effect for a one unit increase in a continuous variable <span class="math inline">\(d\)</span> is given by</p>
<p><span class="math display">\[\frac{1}{N} \sum_{i=1}^{N}\Phi(\alpha + \tau (d_i + 1) + \mathbf{x}_i^{\prime} \beta) - \Phi(\alpha + \tau d_i + \mathbf{x}_i^{\prime} \beta)\]</span></p>
</div>
<div id="section-quantities-of-interest-why" class="section level3">
<h3>Quantities of interest: why?</h3>
<p>The answers are simple:</p>
<ul>
<li>Your readers have a right to know!</li>
<li>It makes your research accessible to non-technical audiences.</li>
<li>Chances are: You will not truly understand your own findings without it.</li>
</ul>
<div id="section-do-not" class="section level4">
<h4>Do not:</h4>
<blockquote>
“The logit-coefficient of our information treatment on turnout is <span class="math inline">\(b = 1.5\)</span> (<span class="math inline">\(p &lt; .05\)</span>). We thus conclude that there is a considerable treatment effect.”
</blockquote>
</div>
<div id="section-do" class="section level4">
<h4>Do:</h4>
<blockquote>
“Our logit model yields a predicted turnout probability of <span class="math inline">\(0.88\)</span> <span class="math inline">\([0.85, 0.91]\)</span> in the treatment group, compared to <span class="math inline">\(0.63\)</span> <span class="math inline">\([0.59, 0.67]\)</span> in the control group. The corresponding difference of <span class="math inline">\(0.25\)</span> <span class="math inline">\([0.22, 0.28]\)</span> is of considerable substantive magnitude and statistically significant at the 95% level.”
</blockquote>
</div>
</div>
</div>
<div id="section-the-simulation-approach" class="section level2">
<h2>The simulation approach</h2>
<div id="section-inferential-uncertainty-vs-fundamental-uncertainty" class="section level3">
<h3>Inferential uncertainty vs fundamental uncertainty</h3>
<p><a href="https://gking.harvard.edu/files/gking/files/making.pdf">King et al. (2000)</a> distinguish <em>inferential uncertainty</em> and <em>fundamental uncertainty</em>.</p>
<ul>
<li><em>Inferential uncertainty</em> describes the problem that we never know our parameters exactly.
<ul>
<li>Instead, we estimate them with some uncertainty that is represented in their respective <em>sampling distribution</em>.</li>
<li>Example: In large samples, we assume that the mean of age in Germany has a normal sampling distribution such that <span class="math inline">\(\hat \mu \sim \text{N}(\bar x, \sigma_{\bar x}^2)\)</span>, where <span class="math inline">\(\sigma_{\bar x}\)</span> is the standard error of the mean.</li>
<li>Inferential uncertainty thus describes our uncertainty about <em>estimates</em> via <em>sampling distributions</em>.</li>
</ul></li>
<li><em>Fundamental uncertainty</em> describes the randomness that comes from the fact that we stipulate stochastic data-generating processes.
<ul>
<li>Example: Over and beyond our uncertainty regarding the true mean of age in Germany, we have a stochastic model in mind that generates age, e.g.: <span class="math inline">\(y \sim \text{N}(\hat \mu, \hat\sigma^2)\)</span>. So we could generate a predictive distribution of age from a normal distribution whose mean is the sample mean and whose variance is the sample variance.</li>
<li>Fundamental uncertainty this describes our uncertainty about <em>predictions</em> via <em>predictive distributions</em>.</li>
</ul></li>
</ul>
</div>
<div id="section-the-glm-context" class="section level3">
<h3>The GLM context</h3>
<p>For a generic GLM, this means:</p>
<p><span class="math display">\[\underbrace{\underbrace{y_i \sim \text{f}}_{\text{fund.}}(\theta_i = g^{-1}(\mathbf{x}_i^{\prime} \underbrace{\beta), \psi}_{\text{inf.}})}_{\text{Total uncertainty}}\]</span></p>
<ol style="list-style-type: decimal">
<li>Inferential uncertainty about the model parameters can be simulated by taking draws from their joint sampling distribution.</li>
<li>Fundamental uncertainty can be simulated by taking draws of <span class="math inline">\(\mathbf{y}\)</span> from <span class="math inline">\(\mathbf{y} \sim f(\theta, \psi)\)</span>.</li>
<li>Total uncertainty can be simulated by simulating (1) within (2).</li>
</ol>
</div>
<div id="section-inferential-uncertainty-in-glm" class="section level3">
<h3>Inferential uncertainty in GLM</h3>
<p>Quantities of interest such as expected values, (average) marginal effects, or (average) first differences are estimates of population parameters.</p>
<p>Unless we engage in predictive modeling, we thus care primarily about inferential uncertainty.</p>
<p>However:</p>
<ul>
<li>In GLM, we rarely estimate our quantities of interest directly.</li>
<li>We usually estimate our <em>model coefficients</em> <span class="math inline">\(\beta\)</span>, from which we derive our quantities of interest.</li>
<li>We thus only have information on the sampling distribution of <span class="math inline">\(\beta\)</span>, <span class="math inline">\(\beta \sim \text{MVN}(\hat\beta, \hat \Sigma)\)</span>, not the sampling distribution of our quantity of interest.</li>
</ul>
<p>So how can we get there?</p>
<ul>
<li>Beyond OLS, deriving the sampling distribution of quantities of interest analytically is rather painful.</li>
<li>Analytical normal-approximation confidence intervals are also imprecise when the asymptotic properties of estimators do not hold in finite samples (e.g., 95% confidence intervals that include predicted probabilities outside of <span class="math inline">\([0, 1]\)</span>).</li>
<li>So we will use a flexible approach that allows us to get sampling distributions for <em>any</em> quantity of interest: <strong>Parameter simulation</strong>.</li>
</ul>
</div>
<div id="section-the-algorithm" class="section level3">
<h3>The algorithm</h3>
<p>The algorithm presented by <a href="https://gking.harvard.edu/files/gking/files/making.pdf">King et al. (2000)</a> contains five steps:</p>
<ol style="list-style-type: decimal">
<li>Simulate the sampling distribution of the model parameters by taking <span class="math inline">\(S\)</span> draws from <span class="math inline">\(\beta \sim \text{MVN}(\hat\beta, \hat \Sigma)\)</span></li>
<li>Choose a vector <span class="math inline">\(\mathbf{x}\)</span> or matrix <span class="math inline">\(\mathbf{X}\)</span>.</li>
<li>For each simulation <span class="math inline">\(\beta^{s}\)</span> for <span class="math inline">\(s=1,...,S\)</span>, calculate <span class="math inline">\(\theta^{s} = g^{-1}(\mathbf{x}^{\prime} \beta^{s})\)</span>.</li>
</ol>
<p>Whenever <span class="math inline">\(\theta\)</span> gives a direct estimate of <span class="math inline">\(\mathbb{E}[y|\mathbf{x}]\)</span>, these three steps will be sufficient!</p>
<p>In some instances involving non-symmetrical transformations, we need to go the extra mile:</p>
<ol start="4" style="list-style-type: decimal">
<li>Simulate the predictive distribution <span class="math inline">\(M\)</span> times for each simulation <span class="math inline">\(\theta^{s}\)</span>.</li>
<li>Average over the <span class="math inline">\(M\)</span> predictive draws within each simulation <span class="math inline">\(s\)</span>.</li>
</ol>
<p>We will <em>not</em> cover any such examples. So in our upcoming applied logistic regression example, we can produce inferential uncertainty with the simpler three-step algorithm.</p>
<p>You can easily see why this the case: The expected value of a Bernoulli distribution with parameter <span class="math inline">\(\pi\)</span> is <span class="math inline">\(\pi\)</span> itself: <span class="math inline">\(\mathbb{E}[\text{Bernoulli}(\pi)] = \pi\)</span>. So drawing many 0’s and 1’s in step 4 just to average them back to a proportion that will be (approximately) equal to <span class="math inline">\(\pi\)</span> does not change anything.</p>

<script type="application/shiny-prerendered" data-context="server-start">
## --- learnr ---
if ("learnr" %in% (.packages()))
  detach(package:learnr, unload = TRUE)
library(learnr)
knitr::opts_chunk$set(echo = FALSE)

## ---- CRAN Packages ----
## Save package names as a vector of strings
pkgs <-  c()

## Install uninstalled packages
lapply(pkgs[!(pkgs %in% installed.packages())], 
       install.packages,
       repos='http://cran.us.r-project.org')

## Load all packages to library and adjust options
lapply(pkgs, library, character.only = TRUE)

## ---- GitHub Packages ----


## ---- Global learnr Objects ----

</script>
 
<script type="application/shiny-prerendered" data-context="server">
learnr:::register_http_handlers(session, metadata = NULL)
</script>
 
<script type="application/shiny-prerendered" data-context="server">
session$onSessionEnded(function() {
        learnr:::session_stop_event(session)
      })
</script>
 <!--html_preserve-->
<script type="application/shiny-prerendered" data-context="dependencies">
{"type":"list","attributes":{},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["jquery"]},{"type":"character","attributes":{},"value":["1.11.3"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/jquery"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["jquery.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["2.7"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["bootstrap"]},{"type":"character","attributes":{},"value":["3.3.5"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/bootstrap"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["viewport"]}},"value":[{"type":"character","attributes":{},"value":["width=device-width, initial-scale=1"]}]},{"type":"character","attributes":{},"value":["js/bootstrap.min.js","shim/html5shiv.min.js","shim/respond.min.js"]},{"type":"character","attributes":{},"value":["css/cerulean.min.css"]},{"type":"character","attributes":{},"value":["<style>h1 {font-size: 34px;}\n       h1.title {font-size: 38px;}\n       h2 {font-size: 30px;}\n       h3 {font-size: 24px;}\n       h4 {font-size: 18px;}\n       h5 {font-size: 16px;}\n       h6 {font-size: 12px;}\n       code {color: inherit; background-color: rgba(0, 0, 0, 0.04);}\n       pre:not([class]) { background-color: white }<\/style>"]},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["2.7"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["pagedtable"]},{"type":"character","attributes":{},"value":["1.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/pagedtable-1.1"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["js/pagedtable.js"]},{"type":"character","attributes":{},"value":["css/pagedtable.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["2.7"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["highlightjs"]},{"type":"character","attributes":{},"value":["9.12.0"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/highlightjs"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["highlight.js"]},{"type":"character","attributes":{},"value":["textmate.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["2.7"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["tutorial"]},{"type":"character","attributes":{},"value":["0.10.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/tutorial"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tutorial.js"]},{"type":"character","attributes":{},"value":["tutorial.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["tutorial-autocompletion"]},{"type":"character","attributes":{},"value":["0.10.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/tutorial"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tutorial-autocompletion.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["tutorial-diagnostics"]},{"type":"character","attributes":{},"value":["0.10.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/tutorial"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tutorial-diagnostics.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["tutorial-format"]},{"type":"character","attributes":{},"value":["0.10.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmarkdown/templates/tutorial/resources"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tutorial-format.js"]},{"type":"character","attributes":{},"value":["tutorial-format.css","rstudio-theme.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["jquery"]},{"type":"character","attributes":{},"value":["1.11.3"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/jquery"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["jquery.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["2.7"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["navigation"]},{"type":"character","attributes":{},"value":["1.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/navigation-1.1"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tabsets.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["2.7"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["highlightjs"]},{"type":"character","attributes":{},"value":["9.12.0"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/highlightjs"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["highlight.js"]},{"type":"character","attributes":{},"value":["default.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["2.7"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["jquery"]},{"type":"character","attributes":{},"value":["1.11.3"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/jquery"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["jquery.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["2.7"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["font-awesome"]},{"type":"character","attributes":{},"value":["5.1.0"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["rmd/h/fontawesome"]}]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["css/all.css","css/v4-shims.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["rmarkdown"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["2.7"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["bootbox"]},{"type":"character","attributes":{},"value":["4.4.0"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/bootbox"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["bootbox.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["idb-keyvalue"]},{"type":"character","attributes":{},"value":["3.2.0"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/idb-keyval"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["idb-keyval-iife-compat.min.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[false]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["tutorial"]},{"type":"character","attributes":{},"value":["0.10.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/tutorial"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tutorial.js"]},{"type":"character","attributes":{},"value":["tutorial.css"]},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["tutorial-autocompletion"]},{"type":"character","attributes":{},"value":["0.10.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/tutorial"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tutorial-autocompletion.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["name","version","src","meta","script","stylesheet","head","attachment","package","all_files","pkgVersion"]},"class":{"type":"character","attributes":{},"value":["html_dependency"]}},"value":[{"type":"character","attributes":{},"value":["tutorial-diagnostics"]},{"type":"character","attributes":{},"value":["0.10.1"]},{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["file"]}},"value":[{"type":"character","attributes":{},"value":["lib/tutorial"]}]},{"type":"NULL"},{"type":"character","attributes":{},"value":["tutorial-diagnostics.js"]},{"type":"NULL"},{"type":"NULL"},{"type":"NULL"},{"type":"character","attributes":{},"value":["learnr"]},{"type":"logical","attributes":{},"value":[true]},{"type":"character","attributes":{},"value":["0.10.1"]}]}]}
</script>
<!--/html_preserve-->
<!--html_preserve-->
<script type="application/shiny-prerendered" data-context="execution_dependencies">
{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["packages"]}},"value":[{"type":"list","attributes":{"names":{"type":"character","attributes":{},"value":["packages","version"]},"class":{"type":"character","attributes":{},"value":["data.frame"]},"row.names":{"type":"integer","attributes":{},"value":[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38]}},"value":[{"type":"character","attributes":{},"value":["base","bslib","compiler","datasets","digest","evaluate","fastmap","graphics","grDevices","htmltools","htmlwidgets","httpuv","jquerylib","jsonlite","knitr","later","learnr","magrittr","markdown","methods","mime","promises","R6","Rcpp","rlang","rmarkdown","rprojroot","sass","shiny","stats","stringi","stringr","tools","utils","withr","xfun","xtable","yaml"]},{"type":"character","attributes":{},"value":["4.0.2","0.2.4","4.0.2","4.0.2","0.6.27","0.14","1.0.1","4.0.2","4.0.2","0.5.1.1","1.5.2","1.5.4","0.1.3","1.7.2","1.31","1.1.0.1","0.10.1","2.0.1","1.1","4.0.2","0.10","1.1.1","2.5.0","1.0.5","0.4.10","2.7","2.0.2","0.3.1","1.5.0","4.0.2","1.5.3","1.4.0","4.0.2","4.0.2","2.3.0","0.21","1.8-4","2.2.1"]}]}]}
</script>
<!--/html_preserve-->
</div>
</div>

</div> <!-- topics -->

<div class="topicsContainer">
<div class="topicsPositioner">
<div class="band">
<div class="bandContent topicsListContainer">

<!-- begin doc-metadata -->
<div id="doc-metadata">
<h2 class="title toc-ignore" style="display:none;">Lecture: Generalized Linear Models</h2>
</div>
<!-- end doc-metadata -->

</div> <!-- bandContent.topicsListContainer -->
</div> <!-- band -->
</div> <!-- topicsPositioner -->
</div> <!-- topicsContainer -->


</div> <!-- bandContent page -->
</div> <!-- pageContent band -->




<script>
// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});
</script>


<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>


</body>

</html>
